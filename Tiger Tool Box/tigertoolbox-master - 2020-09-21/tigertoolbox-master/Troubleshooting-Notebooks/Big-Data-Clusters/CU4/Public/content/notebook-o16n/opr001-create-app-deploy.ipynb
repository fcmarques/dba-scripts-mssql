{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "OPR001 - Create app-deploy\n",
                "==========================\n",
                "\n",
                "Description\n",
                "-----------\n",
                "\n",
                "### Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import getpass\n",
                "\n",
                "app_name = \"app-\" + getpass.getuser().lower()\n",
                "app_version = \"v1\"\n",
                "\n",
                "notebooks = [\n",
                "    os.path.join(\"..\", \"notebook-runner\", \"run505a-sample-notebook.ipynb\") \n",
                "]\n",
                "\n",
                "app_requirements_txt = \"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Set the `azdata` logging directory\n",
                "\n",
                "To support running multiple creates at the same time, place the\n",
                "azdata.log separately. This code is placed here, so it runs after\n",
                "\u2018injected parameters\u2019 (which may change the app\\_name/app\\_version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ[\"AZDATA_LOGGING_LOG_DIR\"] = f\"{app_name}-{app_version}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Marshall injected parameters of list type from string type\n",
                "\n",
                "`list` type arguments passed via `azdata notebook run --arguments` are\n",
                "passed as `string` and need to be converted back to `list` type."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Escape Codes `btnar` need to be further escaped. e.g. if \\t isn't turned into \\\\t, then the \"root\\folder\\tFilename\" becomes \"root\\folder    Filename\".  Escaping \\t \\b \\r \\b \\a as per:\n",
                "#\n",
                "# - http://www.java2s.com/Code/Python/String/EscapeCodesbtnar.htm\n",
                "\n",
                "if isinstance(notebooks, str):\n",
                "    notebooks = notebooks[1:-1].replace('\\t', '\\\\t').replace('\\n', '\\\\n').replace('\\r', '\\\\r').replace('\\b', '\\\\b').replace('\\a', '\\\\a').split(', ')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Common functions\n",
                "\n",
                "Define helper functions used in this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Define `run` function for transient fault handling, suggestions on error, and scrolling updates on Windows\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "import json\n",
                "import platform\n",
                "import shlex\n",
                "import shutil\n",
                "import datetime\n",
                "\n",
                "from subprocess import Popen, PIPE\n",
                "from IPython.display import Markdown\n",
                "\n",
                "retry_hints = {} # Output in stderr known to be transient, therefore automatically retry\n",
                "error_hints = {} # Output in stderr where a known SOP/TSG exists which will be HINTed for further help\n",
                "install_hint = {} # The SOP to help install the executable if it cannot be found\n",
                "\n",
                "first_run = True\n",
                "rules = None\n",
                "debug_logging = False\n",
                "\n",
                "def run(cmd, return_output=False, no_output=False, retry_count=0):\n",
                "    \"\"\"Run shell command, stream stdout, print stderr and optionally return output\n",
                "\n",
                "    NOTES:\n",
                "\n",
                "    1.  Commands that need this kind of ' quoting on Windows e.g.:\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='data-pool')].metadata.name}\n",
                "\n",
                "        Need to actually pass in as '\"':\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='\"'data-pool'\"')].metadata.name}\n",
                "\n",
                "        The ' quote approach, although correct when pasting into Windows cmd, will hang at the line:\n",
                "        \n",
                "            `iter(p.stdout.readline, b'')`\n",
                "\n",
                "        The shlex.split call does the right thing for each platform, just use the '\"' pattern for a '\n",
                "    \"\"\"\n",
                "    MAX_RETRIES = 5\n",
                "    output = \"\"\n",
                "    retry = False\n",
                "\n",
                "    global first_run\n",
                "    global rules\n",
                "\n",
                "    if first_run:\n",
                "        first_run = False\n",
                "        rules = load_rules()\n",
                "\n",
                "    # When running `azdata sql query` on Windows, replace any \\n in \"\"\" strings, with \" \", otherwise we see:\n",
                "    #\n",
                "    #    ('HY090', '[HY090] [Microsoft][ODBC Driver Manager] Invalid string or buffer length (0) (SQLExecDirectW)')\n",
                "    #\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"azdata sql query\"):\n",
                "        cmd = cmd.replace(\"\\n\", \" \")\n",
                "\n",
                "    # shlex.split is required on bash and for Windows paths with spaces\n",
                "    #\n",
                "    cmd_actual = shlex.split(cmd)\n",
                "\n",
                "    # Store this (i.e. kubectl, python etc.) to support binary context aware error_hints and retries\n",
                "    #\n",
                "    user_provided_exe_name = cmd_actual[0].lower()\n",
                "\n",
                "    # When running python, use the python in the ADS sandbox ({sys.executable})\n",
                "    #\n",
                "    if cmd.startswith(\"python \"):\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"python\", sys.executable)\n",
                "\n",
                "        # On Mac, when ADS is not launched from terminal, LC_ALL may not be set, which causes pip installs to fail\n",
                "        # with:\n",
                "        #\n",
                "        #    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4969: ordinal not in range(128)\n",
                "        #\n",
                "        # Setting it to a default value of \"en_US.UTF-8\" enables pip install to complete\n",
                "        #\n",
                "        if platform.system() == \"Darwin\" and \"LC_ALL\" not in os.environ:\n",
                "            os.environ[\"LC_ALL\"] = \"en_US.UTF-8\"\n",
                "\n",
                "    # When running `kubectl`, if AZDATA_OPENSHIFT is set, use `oc`\n",
                "    #\n",
                "    if cmd.startswith(\"kubectl \") and \"AZDATA_OPENSHIFT\" in os.environ:\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"kubectl\", \"oc\")\n",
                "\n",
                "    # To aid supportabilty, determine which binary file will actually be executed on the machine\n",
                "    #\n",
                "    which_binary = None\n",
                "\n",
                "    # Special case for CURL on Windows.  The version of CURL in Windows System32 does not work to\n",
                "    # get JWT tokens, it returns \"(56) Failure when receiving data from the peer\".  If another instance\n",
                "    # of CURL exists on the machine use that one.  (Unfortunately the curl.exe in System32 is almost\n",
                "    # always the first curl.exe in the path, and it can't be uninstalled from System32, so here we\n",
                "    # look for the 2nd installation of CURL in the path)\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"curl \"):\n",
                "        path = os.getenv('PATH')\n",
                "        for p in path.split(os.path.pathsep):\n",
                "            p = os.path.join(p, \"curl.exe\")\n",
                "            if os.path.exists(p) and os.access(p, os.X_OK):\n",
                "                if p.lower().find(\"system32\") == -1:\n",
                "                    cmd_actual[0] = p\n",
                "                    which_binary = p\n",
                "                    break\n",
                "\n",
                "    # Find the path based location (shutil.which) of the executable that will be run (and display it to aid supportability), this\n",
                "    # seems to be required for .msi installs of azdata.cmd/az.cmd.  (otherwise Popen returns FileNotFound) \n",
                "    #\n",
                "    # NOTE: Bash needs cmd to be the list of the space separated values hence shlex.split.\n",
                "    #\n",
                "    if which_binary == None:\n",
                "        which_binary = shutil.which(cmd_actual[0])\n",
                "\n",
                "    if which_binary == None:\n",
                "        if user_provided_exe_name in install_hint and install_hint[user_provided_exe_name] is not None:\n",
                "            display(Markdown(f'HINT: Use [{install_hint[user_provided_exe_name][0]}]({install_hint[user_provided_exe_name][1]}) to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\")\n",
                "    else:   \n",
                "        cmd_actual[0] = which_binary\n",
                "\n",
                "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
                "\n",
                "    print(f\"START: {cmd} @ {start_time} ({datetime.datetime.utcnow().replace(microsecond=0)} UTC)\")\n",
                "    print(f\"       using: {which_binary} ({platform.system()} {platform.release()} on {platform.machine()})\")\n",
                "    print(f\"       cwd: {os.getcwd()}\")\n",
                "\n",
                "    # Command-line tools such as CURL and AZDATA HDFS commands output\n",
                "    # scrolling progress bars, which causes Jupyter to hang forever, to\n",
                "    # workaround this, use no_output=True\n",
                "    #\n",
                "\n",
                "    # Work around a infinite hang when a notebook generates a non-zero return code, break out, and do not wait\n",
                "    #\n",
                "    wait = True \n",
                "\n",
                "    try:\n",
                "        if no_output:\n",
                "            p = Popen(cmd_actual)\n",
                "        else:\n",
                "            p = Popen(cmd_actual, stdout=PIPE, stderr=PIPE, bufsize=1)\n",
                "            with p.stdout:\n",
                "                for line in iter(p.stdout.readline, b''):\n",
                "                    line = line.decode()\n",
                "                    if return_output:\n",
                "                        output = output + line\n",
                "                    else:\n",
                "                        if cmd.startswith(\"azdata notebook run\"): # Hyperlink the .ipynb file\n",
                "                            regex = re.compile('  \"(.*)\"\\: \"(.*)\"') \n",
                "                            match = regex.match(line)\n",
                "                            if match:\n",
                "                                if match.group(1).find(\"HTML\") != -1:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"{match.group(2)}\"'))\n",
                "                                else:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"[{match.group(2)}]({match.group(2)})\"'))\n",
                "\n",
                "                                    wait = False\n",
                "                                    break # otherwise infinite hang, have not worked out why yet.\n",
                "                        else:\n",
                "                            print(line, end='')\n",
                "                            if rules is not None:\n",
                "                                apply_expert_rules(line)\n",
                "\n",
                "        if wait:\n",
                "            p.wait()\n",
                "    except FileNotFoundError as e:\n",
                "        if install_hint is not None:\n",
                "            display(Markdown(f'HINT: Use {install_hint} to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\") from e\n",
                "\n",
                "    exit_code_workaround = 0 # WORKAROUND: azdata hangs on exception from notebook on p.wait()\n",
                "\n",
                "    if not no_output:\n",
                "        for line in iter(p.stderr.readline, b''):\n",
                "            try:\n",
                "                line_decoded = line.decode()\n",
                "            except UnicodeDecodeError:\n",
                "                # NOTE: Sometimes we get characters back that cannot be decoded(), e.g.\n",
                "                #\n",
                "                #   \\xa0\n",
                "                #\n",
                "                # For example see this in the response from `az group create`:\n",
                "                #\n",
                "                # ERROR: Get Token request returned http error: 400 and server \n",
                "                # response: {\"error\":\"invalid_grant\",# \"error_description\":\"AADSTS700082: \n",
                "                # The refresh token has expired due to inactivity.\\xa0The token was \n",
                "                # issued on 2018-10-25T23:35:11.9832872Z\n",
                "                #\n",
                "                # which generates the exception:\n",
                "                #\n",
                "                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 179: invalid start byte\n",
                "                #\n",
                "                print(\"WARNING: Unable to decode stderr line, printing raw bytes:\")\n",
                "                print(line)\n",
                "                line_decoded = \"\"\n",
                "                pass\n",
                "            else:\n",
                "\n",
                "                # azdata emits a single empty line to stderr when doing an hdfs cp, don't\n",
                "                # print this empty \"ERR:\" as it confuses.\n",
                "                #\n",
                "                if line_decoded == \"\":\n",
                "                    continue\n",
                "                \n",
                "                print(f\"STDERR: {line_decoded}\", end='')\n",
                "\n",
                "                if line_decoded.startswith(\"An exception has occurred\") or line_decoded.startswith(\"ERROR: An error occurred while executing the following cell\"):\n",
                "                    exit_code_workaround = 1\n",
                "\n",
                "                # inject HINTs to next TSG/SOP based on output in stderr\n",
                "                #\n",
                "                if user_provided_exe_name in error_hints:\n",
                "                    for error_hint in error_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(error_hint[0]) != -1:\n",
                "                            display(Markdown(f'HINT: Use [{error_hint[1]}]({error_hint[2]}) to resolve this issue.'))\n",
                "\n",
                "                # apply expert rules (to run follow-on notebooks), based on output\n",
                "                #\n",
                "                if rules is not None:\n",
                "                    apply_expert_rules(line_decoded)\n",
                "\n",
                "                # Verify if a transient error, if so automatically retry (recursive)\n",
                "                #\n",
                "                if user_provided_exe_name in retry_hints:\n",
                "                    for retry_hint in retry_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(retry_hint) != -1:\n",
                "                            if retry_count < MAX_RETRIES:\n",
                "                                print(f\"RETRY: {retry_count} (due to: {retry_hint})\")\n",
                "                                retry_count = retry_count + 1\n",
                "                                output = run(cmd, return_output=return_output, retry_count=retry_count)\n",
                "\n",
                "                                if return_output:\n",
                "                                    return output\n",
                "                                else:\n",
                "                                    return\n",
                "\n",
                "    elapsed = datetime.datetime.now().replace(microsecond=0) - start_time\n",
                "\n",
                "    # WORKAROUND: We avoid infinite hang above in the `azdata notebook run` failure case, by inferring success (from stdout output), so\n",
                "    # don't wait here, if success known above\n",
                "    #\n",
                "    if wait: \n",
                "        if p.returncode != 0:\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(p.returncode)}.\\n')\n",
                "    else:\n",
                "        if exit_code_workaround !=0 :\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(exit_code_workaround)}.\\n')\n",
                "\n",
                "    print(f'\\nSUCCESS: {elapsed}s elapsed.\\n')\n",
                "\n",
                "    if return_output:\n",
                "        return output\n",
                "\n",
                "def load_json(filename):\n",
                "    \"\"\"Load a json file from disk and return the contents\"\"\"\n",
                "\n",
                "    with open(filename, encoding=\"utf8\") as json_file:\n",
                "        return json.load(json_file)\n",
                "\n",
                "def load_rules():\n",
                "    \"\"\"Load any 'expert rules' from the metadata of this notebook (.ipynb) that should be applied to the stderr of the running executable\"\"\"\n",
                "\n",
                "    # Load this notebook as json to get access to the expert rules in the notebook metadata.\n",
                "    #\n",
                "    try:\n",
                "        j = load_json(\"opr001-create-app-deploy.ipynb\")\n",
                "    except:\n",
                "        pass # If the user has renamed the book, we can't load ourself.  NOTE: Is there a way in Jupyter, to know your own filename?\n",
                "    else:\n",
                "        if \"metadata\" in j and \\\n",
                "            \"azdata\" in j[\"metadata\"] and \\\n",
                "            \"expert\" in j[\"metadata\"][\"azdata\"] and \\\n",
                "            \"expanded_rules\" in j[\"metadata\"][\"azdata\"][\"expert\"]:\n",
                "\n",
                "            rules = j[\"metadata\"][\"azdata\"][\"expert\"][\"expanded_rules\"]\n",
                "\n",
                "            rules.sort() # Sort rules, so they run in priority order (the [0] element).  Lowest value first.\n",
                "\n",
                "            # print (f\"EXPERT: There are {len(rules)} rules to evaluate.\")\n",
                "\n",
                "            return rules\n",
                "\n",
                "def apply_expert_rules(line):\n",
                "    \"\"\"Determine if the stderr line passed in, matches the regular expressions for any of the 'expert rules', if so\n",
                "    inject a 'HINT' to the follow-on SOP/TSG to run\"\"\"\n",
                "\n",
                "    global rules\n",
                "\n",
                "    for rule in rules:\n",
                "        notebook = rule[1]\n",
                "        cell_type = rule[2]\n",
                "        output_type = rule[3] # i.e. stream or error\n",
                "        output_type_name = rule[4] # i.e. ename or name \n",
                "        output_type_value = rule[5] # i.e. SystemExit or stdout\n",
                "        details_name = rule[6]  # i.e. evalue or text \n",
                "        expression = rule[7].replace(\"\\\\*\", \"*\") # Something escaped *, and put a \\ in front of it!\n",
                "\n",
                "        if debug_logging:\n",
                "            print(f\"EXPERT: If rule '{expression}' satisfied', run '{notebook}'.\")\n",
                "\n",
                "        if re.match(expression, line, re.DOTALL):\n",
                "\n",
                "            if debug_logging:\n",
                "                print(\"EXPERT: MATCH: name = value: '{0}' = '{1}' matched expression '{2}', therefore HINT '{4}'\".format(output_type_name, output_type_value, expression, notebook))\n",
                "\n",
                "            match_found = True\n",
                "\n",
                "            display(Markdown(f'HINT: Use [{notebook}]({notebook}) to resolve this issue.'))\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "print('Common functions defined successfully.')\n",
                "\n",
                "# Hints for binary (transient fault) retry, (known) error and install guide\n",
                "#\n",
                "retry_hints = {'kubectl': ['A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'], 'azdata': ['Endpoint sql-server-master does not exist', 'Endpoint livy does not exist', 'Failed to get state for cluster', 'Endpoint webhdfs does not exist', 'Adaptive Server is unavailable or does not exist', 'Error: Address already in use']}\n",
                "error_hints = {'kubectl': [['no such host', 'TSG010 - Get configuration contexts', '../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb'], ['No connection could be made because the target machine actively refused it', 'TSG056 - Kubectl fails with No connection could be made because the target machine actively refused it', '../repair/tsg056-kubectl-no-connection-could-be-made.ipynb']], 'azdata': [['azdata login', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['The token is expired', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Reason: Unauthorized', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Max retries exceeded with url: /api/v1/bdc/endpoints', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Look at the controller logs for more details', 'TSG027 - Observe cluster deployment', '../diagnose/tsg027-observe-bdc-create.ipynb'], ['provided port is already allocated', 'TSG062 - Get tail of all previous container logs for pods in BDC namespace', '../log-files/tsg062-tail-bdc-previous-container-logs.ipynb'], ['Create cluster failed since the existing namespace', 'SOP061 - Delete a big data cluster', '../install/sop061-delete-bdc.ipynb'], ['Failed to complete kube config setup', 'TSG067 - Failed to complete kube config setup', '../repair/tsg067-failed-to-complete-kube-config-setup.ipynb'], ['Error processing command: \"ApiError', 'TSG110 - Azdata returns ApiError', '../repair/tsg110-azdata-returns-apierror.ipynb'], ['Error processing command: \"ControllerError', 'TSG036 - Controller logs', '../log-analyzers/tsg036-get-controller-logs.ipynb'], ['ERROR: 500', 'TSG046 - Knox gateway logs', '../log-analyzers/tsg046-get-knox-logs.ipynb'], ['Data source name not found and no default driver specified', 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], [\"Can't open lib 'ODBC Driver 17 for SQL Server\", 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], ['Control plane upgrade failed. Failed to upgrade controller.', 'TSG108 - View the controller upgrade config map', '../diagnose/tsg108-controller-failed-to-upgrade.ipynb']]}\n",
                "install_hint = {'kubectl': ['SOP036 - Install kubectl command line interface', '../install/sop036-install-kubectl.ipynb'], 'azdata': ['SOP063 - Install azdata CLI (using package manager)', '../install/sop063-packman-install-azdata.ipynb']}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Instantiate Kubernetes client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Instantiate the Python Kubernetes client into 'api' variable\n",
                "\n",
                "import os\n",
                "\n",
                "try:\n",
                "    from kubernetes import client, config\n",
                "    from kubernetes.stream import stream\n",
                "\n",
                "    if \"KUBERNETES_SERVICE_PORT\" in os.environ and \"KUBERNETES_SERVICE_HOST\" in os.environ:\n",
                "        config.load_incluster_config()\n",
                "    else:\n",
                "        try:\n",
                "            config.load_kube_config()\n",
                "        except:\n",
                "            display(Markdown(f'HINT: Use [TSG118 - Configure Kubernetes config](../repair/tsg118-configure-kube-config.ipynb) to resolve this issue.'))\n",
                "            raise\n",
                "    api = client.CoreV1Api()\n",
                "\n",
                "    print('Kubernetes client instantiated')\n",
                "except ImportError:\n",
                "    from IPython.display import Markdown\n",
                "    display(Markdown(f'HINT: Use [SOP059 - Install Kubernetes Python module](../install/sop059-install-kubernetes-module.ipynb) to resolve this issue.'))\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get the Kubernetes namespace for the big data cluster\n",
                "\n",
                "Get the namespace of the Big Data Cluster use the kubectl command line\n",
                "interface .\n",
                "\n",
                "**NOTE:**\n",
                "\n",
                "If there is more than one Big Data Cluster in the target Kubernetes\n",
                "cluster, then either:\n",
                "\n",
                "-   set \\[0\\] to the correct value for the big data cluster.\n",
                "-   set the environment variable AZDATA\\_NAMESPACE, before starting\n",
                "    Azure Data Studio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Place Kubernetes namespace name for BDC into 'namespace' variable\n",
                "\n",
                "if \"AZDATA_NAMESPACE\" in os.environ:\n",
                "    namespace = os.environ[\"AZDATA_NAMESPACE\"]\n",
                "else:\n",
                "    try:\n",
                "        namespace = run(f'kubectl get namespace --selector=MSSQL_CLUSTER -o jsonpath={{.items[0].metadata.name}}', return_output=True)\n",
                "    except:\n",
                "        from IPython.display import Markdown\n",
                "        print(f\"ERROR: Unable to find a Kubernetes namespace with label 'MSSQL_CLUSTER'.  SQL Server Big Data Cluster Kubernetes namespaces contain the label 'MSSQL_CLUSTER'.\")\n",
                "        display(Markdown(f'HINT: Use [TSG081 - Get namespaces (Kubernetes)](../monitor-k8s/tsg081-get-kubernetes-namespaces.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [TSG010 - Get configuration contexts](../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [SOP011 - Set kubernetes configuration context](../common/sop011-set-kubernetes-context.ipynb) to resolve this issue.'))\n",
                "        raise\n",
                "\n",
                "print(f'The SQL Server Big Data Cluster Kubernetes namespace is: {namespace}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create a temporary directory to stage files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Create a temporary directory to hold configuration files\n",
                "\n",
                "import tempfile\n",
                "\n",
                "temp_dir = tempfile.mkdtemp()\n",
                "\n",
                "print(f\"Temporary directory created: {temp_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Copy notebook files to app-deploy staging folder\n",
                "\n",
                "Copy notebooks from library into staging directory. The notebooks are\n",
                "prefixed with \u2018step001\u2019 etc. so their order is maintained and they will\n",
                "be run in order.\n",
                "\n",
                "Each notebook may also contain `expert rules` whose expression if\n",
                "matched in the output of a notebook, requires further notebooks to be\n",
                "run. Those additional notebooks will be staged as well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "from shutil import copyfile\n",
                "\n",
                "# Always copy in run001, it's used to run all other notebooks\n",
                "#\n",
                "additional_notebooks = [\n",
                "    os.path.join(\"..\", \"notebook-runner\", \"run001-run-notebook.ipynb\"),\n",
                "    os.path.join(\"..\", \"notebook-runner\", \"run002-save-result-in-bdc.ipynb\"),\n",
                "    os.path.join(\"..\", \"notebook-runner\", \"run003-run-expert-rules.ipynb\"),\n",
                "]\n",
                "\n",
                "def load_json(filename):\n",
                "    with open(filename, encoding=\"utf8\") as json_file:\n",
                "        return json.load(json_file)\n",
                "\n",
                "def look_for_additional_notebooks(n):\n",
                "    \"\"\"Check to see if this notebook has expert rules requiring other notebooks that need to be run (on expression match)\n",
                "    \"\"\"\n",
                "    j = load_json(n)\n",
                "\n",
                "    if \"metadata\" in j and \\\n",
                "        \"azdata\" in j[\"metadata\"] and \\\n",
                "        \"expert\" in j[\"metadata\"][\"azdata\"] and \\\n",
                "        \"expanded_rules\" in j[\"metadata\"][\"azdata\"][\"expert\"]:\n",
                "\n",
                "        rules = j[\"metadata\"][\"azdata\"][\"expert\"][\"expanded_rules\"]\n",
                "\n",
                "        for rule in rules:\n",
                "            additional_notebook = rule[1]\n",
                "            additional_notebooks.append(additional_notebook)\n",
                "\n",
                "            # Recursively continue looking for additional notebooks in the expert rules\n",
                "            #\n",
                "            look_for_additional_notebooks(additional_notebook)\n",
                "\n",
                "step_counter = 1\n",
                "for notebook in notebooks:\n",
                "    destination_file = \"step{0:03d}-{1}\".format(step_counter, os.path.basename(notebook))\n",
                "    copyfile(notebook, os.path.join(temp_dir, destination_file))\n",
                "    print(\"Staged: \" + destination_file)\n",
                "\n",
                "    look_for_additional_notebooks(notebook)\n",
                "    \n",
                "    step_counter = step_counter + 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Copy additional notebook files to app-deploy staging folder\n",
                "\n",
                "A notebook may contain `expert rules` which contain expressions, which\n",
                "if matched in the output of a notebook, require further additional\n",
                "notebooks to be run. Copy those additional notebooks into the staging\n",
                "directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from shutil import copyfile\n",
                "\n",
                "for notebook in additional_notebooks:\n",
                "    copyfile(notebook, os.path.join(temp_dir, os.path.basename(notebook)))\n",
                "    print(\"Staged: \" + os.path.basename(notebook))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper function to save configuration files to disk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Define helper function 'save_file' to save configuration files to the temporary directory created above\n",
                "import os\n",
                "import io\n",
                "\n",
                "def save_file(filename, contents):\n",
                "    with io.open(os.path.join(temp_dir, filename), \"w\", encoding='utf8', newline='\\n') as text_file:\n",
                "      text_file.write(contents)\n",
                "\n",
                "      print(\"File saved: \" + os.path.join(temp_dir, filename))\n",
                "\n",
                "print(\"Function `save_file` defined successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get the controller username and password\n",
                "\n",
                "Get the controller username and password from the Kubernetes Secret\n",
                "Store and place in the required AZDATA\\_USERNAME and AZDATA\\_PASSWORD\n",
                "environment variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Place controller secret in AZDATA_USERNAME/AZDATA_PASSWORD environment variables\n",
                "\n",
                "import os, base64\n",
                "\n",
                "os.environ[\"AZDATA_USERNAME\"] = run(f'kubectl get secret/controller-login-secret -n {namespace} -o jsonpath={{.data.username}}', return_output=True)\n",
                "os.environ[\"AZDATA_USERNAME\"] = base64.b64decode(os.environ[\"AZDATA_USERNAME\"]).decode('utf-8')\n",
                "\n",
                "os.environ[\"AZDATA_PASSWORD\"] = run(f'kubectl get secret/controller-login-secret -n {namespace} -o jsonpath={{.data.password}}', return_output=True)\n",
                "os.environ[\"AZDATA_PASSWORD\"] = base64.b64decode(os.environ[\"AZDATA_PASSWORD\"]).decode('utf-8')\n",
                "\n",
                "\n",
                "print(f\"Controller username '{os.environ['AZDATA_USERNAME']}' and password stored in environment variables\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Configure the app-deploy azdata python dependencies\n",
                "\n",
                "NOTE: These dependencies are installed into `azdata` python installation\n",
                "in the app-deploy container, so they are available to\n",
                "`azdata notebook run`.\n",
                "\n",
                "NOTE: azdata notebook run does not support injecting arguments with \"\"\"\n",
                "(which can encode newlines), therefore treat \" \" as a need for a newline\n",
                "(and replace)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_file(\"app-requirements.txt\", app_requirements_txt.replace(\" \", \"\\n\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Configure container scoped dependencies\n",
                "\n",
                "These are the dependencies that need to be installed before Python pip\n",
                "install is called (requirements.txt)\n",
                "\n",
                "-   azdata\n",
                "-   spark kernels (into azdata python instance)\n",
                "-   kubectl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pre_package_install_sh = \"\"\"\n",
                "apt-get install gnupg ca-certificates curl apt-transport-https lsb-release -y\n",
                "\n",
                "# Install AZDATA\n",
                "#\n",
                "wget -qO- https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
                "add-apt-repository \"$(wget -qO- https://packages.microsoft.com/config/ubuntu/16.04/mssql-server-2019.list)\"\n",
                "apt-get update\n",
                "apt-get install -y azdata-cli\n",
                "azdata --version\n",
                "\n",
                "ACCEPT_EULA=Y apt-get install -y msodbcsql17 unixodbc-dev\n",
                "\n",
                "# Install Spark (Scala) and PySpark Jupyter kernels\n",
                "#\n",
                "/opt/azdata/bin/python3 -m pip install -r /var/opt/app/app-requirements.txt\n",
                "/opt/azdata/bin/python3 -m pip install sparkmagic\n",
                "/opt/azdata/bin/python3 /opt/azdata/bin/jupyter-kernelspec install --user /opt/azdata/lib/python3.6/site-packages/sparkmagic/kernels/sparkkernel\n",
                "/opt/azdata/bin/python3 /opt/azdata/bin/jupyter-kernelspec install --user /opt/azdata/lib/python3.6/site-packages/sparkmagic/kernels/pysparkkernel\n",
                "\n",
                "mkdir -p /root/.sparkmagic\n",
                "echo '{ \"ignore_ssl_errors\": true }' > /root/.sparkmagic/config.json\n",
                "\n",
                "# Install SQL and Powershell notebook kernels\n",
                "#\n",
                "\n",
                "# Download the Microsoft repository GPG keys\n",
                "wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb\n",
                "\n",
                "# Register the Microsoft repository GPG keys\n",
                "dpkg -i packages-microsoft-prod.deb\n",
                "\n",
                "# Update the list of products\n",
                "apt-get update\n",
                "\n",
                "# Install PowerShell\n",
                "apt-get install -y powershell\n",
                "\n",
                "# Install SqlServer module\n",
                "pwsh -Command \"Install-Module -Name SqlServer -force\"\n",
                "\n",
                "# Install KUBECTL\n",
                "#\n",
                "apt-get update && apt-get install -y apt-transport-https\n",
                "curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
                "echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | tee -a /etc/apt/sources.list.d/kubernetes.list\n",
                "apt-get update\n",
                "apt-get install -y kubectl\n",
                "\"\"\"\n",
                "\n",
                "save_file(\"pre-package-install.sh\", pre_package_install_sh)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The app-deploy python script\n",
                "\n",
                "This script runs in the app-deploy container.\n",
                "\n",
                "NOTE: The app-deploy container itself supports only Python 3.5, so\n",
                "features like interpolated strings will not work in the script that\n",
                "app-deploy launches. But the azdata CLI is installed into Python 3.6, so\n",
                "the notebooks that azdata CLI runs can use the later features of Python."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_py = f\"\"\"\n",
                "import os,  json, datetime, shlex, subprocess, glob, base64\n",
                "from subprocess import PIPE, Popen\n",
                "\n",
                "# Retry hints for transient errors that happen before cell execution.  There are more retry hints for cell level\n",
                "# transient errors inside the notebook run() function.\n",
                "#\n",
                "retry_hints = [\n",
                "    \"Error: Address already in use\"  # About .1% this is seen, resulting in RuntimeError: Kernel died before replying to kernel_info\n",
                "]\n",
                "\n",
                "MAX_RETRIES = 10\n",
                "\n",
                "# NOTEBOOK_CELL_TIMEOUT\n",
                "#\n",
                "# 4 hours (14400 minutes) per cell timeout (overrides the 10 minute default, e.g. core dump file \n",
                "# copies can take a very long time.  Note, individual notebooks can have their own\n",
                "# timeout, so this timeout needs to be larger than any individual notebooks. \n",
                "#\n",
                "NOTEBOOK_CELL_TIMEOUT = 14400\n",
                "\n",
                "USE_AD_AUTH = {\"True\" if \"AZDATA_AD_AUTH\" in os.environ else \"False\"}\n",
                "\n",
                "def run(cmd, retry_count=0, return_output=False):\n",
                "\n",
                "    env = os.environ.copy()\n",
                "    env[\"ACCEPT_EULA\"] = \"yes\"\n",
                "\n",
                "    print(\"START:\" + cmd)\n",
                "    process = Popen(shlex.split(cmd), stdout=PIPE, stderr=PIPE, env=env)\n",
                "    stdout, stderr = process.communicate()\n",
                "    returncode = process.returncode\n",
                "\n",
                "    stdout = str(stdout.decode())\n",
                "    if stdout != \"\" and not return_output: # Don't print if returning, may contain secrets\n",
                "        print(\"STDOUT: \" + stdout)\n",
                "\n",
                "    stderr = str(stderr.decode())\n",
                "    if stderr != \"\":\n",
                "        print(\"STDERR: \" + stderr)\n",
                "\n",
                "    for retry_hint in retry_hints:\n",
                "        if stderr.find(retry_hint) != -1:\n",
                "            if retry_count < MAX_RETRIES:\n",
                "                retry_count = retry_count + 1\n",
                "                print(\"RETRY: Retrying {{0}} (due to: {{1}})\".format(retry_count, retry_hint))\n",
                "                return run(cmd, retry_count=retry_count)\n",
                "\n",
                "    print(\"SUCCESS: \" if returncode == 0 else \"ERROR: \" + cmd)\n",
                "\n",
                "    if return_output:\n",
                "        return returncode, stdout\n",
                "    else:\n",
                "        return returncode\n",
                "\n",
                "def handler():\n",
                "\n",
                "    session_start = datetime.datetime.utcnow()\n",
                "\n",
                "    # Remove output-*.ipynb files from previous runs\n",
                "    #\n",
                "    for notebook in glob.glob(\"output-*.ipynb\"):\n",
                "        os.remove(notebook)\n",
                "\n",
                "    # Run notebooks in order\n",
                "    #\n",
                "    files = glob.glob('step*.ipynb')\n",
                "    files.sort()\n",
                "\n",
                "    for file in files:\n",
                "        exit_code = run('azdata notebook run --path \"run001-run-notebook.ipynb\" -a \"{{{{\\\\\\\\\"session_start\\\\\\\\\": \\\\\\\\\"{{0}}\\\\\\\\\", \\\\\\\\\"app_name\\\\\\\\\": \\\\\\\\\"{app_name}\\\\\\\\\", \\\\\\\\\"app_version\\\\\\\\\": \\\\\\\\\"{app_version}\\\\\\\\\", \\\\\\\\\"notebook_path\\\\\\\\\": \\\\\\\\\"{{1}}\\\\\\\\\", \\\\\\\\\"use_ad_auth\\\\\\\\\": \\\\\\\\\"{{4}}\\\\\\\\\"}}}}\" --output-path \"{{2}}\" --output-html --timeout {{3}}'.format(session_start, file, os.getcwd(), NOTEBOOK_CELL_TIMEOUT, \"True\" if USE_AD_AUTH else \"False\"))\n",
                "\n",
                "        if exit_code != 0:\n",
                "\n",
                "            hdfs_destination = 'app-deploy/{{0}}-{{1}}/logs/{{2}}/{{3:02d}}/{{4:02d}}/{{5:02d}}-{{6:02d}}-{{7}}-{{8}}'.format(\n",
                "                \"{app_name}\", \"{app_version}\", session_start.year, session_start.month, session_start.day, session_start.hour, session_start.minute, \"FAILURE\", \"run001-run-notebook.ipynb\" +  \"-\" + file)\n",
                "\n",
                "            _, os.environ[\"AZDATA_USERNAME\"] = run(\"kubectl get secret/controller-login-secret -n {namespace} -o jsonpath={{.data.username}}\", return_output=True)\n",
                "            os.environ[\"AZDATA_USERNAME\"] = base64.b64decode(os.environ[\"AZDATA_USERNAME\"]).decode('utf-8')\n",
                "\n",
                "            _, os.environ[\"AZDATA_PASSWORD\"] = run(\"kubectl get secret/controller-login-secret -n {namespace} -o jsonpath={{.data.password}}\", return_output=True)\n",
                "            os.environ[\"AZDATA_PASSWORD\"] = base64.b64decode(os.environ[\"AZDATA_PASSWORD\"]).decode('utf-8')\n",
                "\n",
                "            run(\"azdata bdc hdfs cp --from-path {{0}} --to-path hdfs:/{{1}}\".format(\n",
                "                os.path.join(os.getcwd(), \"output-run001-run-notebook.ipynb\").replace(\"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), hdfs_destination))\n",
                "\n",
                "            break\n",
                "\n",
                "    return exit_code\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    handler()\n",
                "\"\"\"\n",
                "\n",
                "save_file(\"run.py\", run_py)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Configure the app-deploy spec yaml file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spec = f\"\"\"\n",
                "entrypoint: handler\n",
                "name: {app_name}\n",
                "output:\n",
                "  out: str\n",
                "owners: []\n",
                "poolsize: 1\n",
                "readers: []\n",
                "replicas: 1\n",
                "runtime: Python\n",
                "src: ./run.py\n",
                "version: {app_version}\n",
                "\"\"\"\n",
                "\n",
                "save_file(\"spec.yaml\", spec)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Delete the app-deploy if previously created"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app_exists = run(f\"azdata app list --name {app_name} --version {app_version}\", return_output=True)\n",
                "\n",
                "if len(app_exists) > 2:  # 2 is the length returned when the app does not exist\n",
                "    run(f\"azdata app delete --name {app_name} --version {app_version}\")\n",
                "\n",
                "# NOTE: Provide diagostic information for a 409 result (Conflict) in the next step, run `app list` to verify\n",
                "# the app has been deleted.  409 (Conflict) has been seen below\n",
                "#\n",
                "run(f\"azdata app list --name {app_name} --version {app_version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create the app-deploy application"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run(f'azdata app create --spec \"{temp_dir}\"')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Wait for the app-deploy application to get into Ready state"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "state = \"WaitingForCreate\"\n",
                "counter = 0\n",
                "MAX_CHECKS_FOR_TO_REACH_READY_STATE = 150 # It can take a while for all the requirements to be installed!\n",
                "\n",
                "while state == \"Creating\" or state == \"WaitingForCreate\" or state == \"WaitingForCredentials\":\n",
                "    app_state = run(f'azdata app describe --name {app_name} --version {app_version} --spec \"{temp_dir}\"', return_output=True)\n",
                "\n",
                "    app_state_json = json.loads(app_state)\n",
                "\n",
                "    try:\n",
                "        state = app_state_json[\"state\"]\n",
                "    except TypeError as e: # Sometimes, we see \"TypeError: string indices must be integers\"\n",
                "        state = str(ex)\n",
                "\n",
                "    counter = counter + 1\n",
                "    print(f\"State: '{state}' (waiting ({counter}) for 'Ready' state)\")\n",
                "    print('')\n",
                "\n",
                "    if counter == MAX_CHECKS_FOR_TO_REACH_READY_STATE:\n",
                "        print (app_state)\n",
                "        raise SystemExit(f'App has not moved to Ready state in {counter} attempts')\n",
                "\n",
                "print (f\"App state: {app_state}\")\n",
                "\n",
                "if state == \"Error\":\n",
                "    raise SystemExit('App is in Error state')\n",
                "\n",
                "print (f\"App successfully moved from WaitingForCreate through to Ready state\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Show the app-deploy details"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run(f\"azdata app describe --name {app_name} --version {app_version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create the Kubernetes RBAC settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "role_binding = run(f\"kubectl get clusterrolebindings --field-selector metadata.name={app_name}-{app_version}-{namespace}-default-admin-binding --no-headers -o jsonpath={{.items}}\", return_output=True)\n",
                "\n",
                "if role_binding == \"[]\": # does not exist\n",
                "    run(f\"kubectl create clusterrolebinding {app_name}-{app_version}-{namespace}-default-admin-binding --clusterrole=cluster-admin --user=system:serviceaccount:{namespace}:default\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Verify the app-deploy installation logs\n",
                "\n",
                "The app-deploy installation logs are in the directory:\n",
                "\n",
                "    /var/log/supervisor/log\n",
                "\n",
                "e.g.\n",
                "\n",
                "-   AppSetup-stderr\u2014supervisor-AZYYnr.log\n",
                "-   MLServer-stderr\u2014supervisor-RgvrIA.log\n",
                "-   AppSetup-stdout\u2014supervisor-qsAKzn.log\n",
                "-   MLServer-stdout\u2014supervisor-vRFUc5.log"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "tail_lines = 2000\n",
                "pod = run(f'kubectl get pod --selector=app=app-{app_name}-{app_version} -n {namespace} -o jsonpath={{.items[0].metadata.name}}', return_output=True)\n",
                "container = \"mlserver\"\n",
                "log_files = [ \"/var/log/supervisor/log/*-*---supervisor-*.log\" ]\n",
                "\n",
                "expressions_to_analyze = [ ]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get tail for log"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Display the last 'tail_lines' of files in 'log_files' list\n",
                "\n",
                "pods = api.list_namespaced_pod(namespace)\n",
                "\n",
                "entries_for_analysis = []\n",
                "\n",
                "for p in pods.items:\n",
                "    if pod is None or p.metadata.name == pod:\n",
                "        for c in p.spec.containers:\n",
                "            if container is None or c.name == container:\n",
                "                for log_file in log_files:\n",
                "                    print (f\"- LOGS: '{log_file}' for CONTAINER: '{c.name}' in POD: '{p.metadata.name}'\")\n",
                "                    try:\n",
                "                        output = stream(api.connect_get_namespaced_pod_exec, p.metadata.name, namespace, command=['/bin/sh', '-c', f'tail -n {tail_lines} {log_file}'], container=c.name, stderr=True, stdout=True)\n",
                "                    except Exception:\n",
                "                        print (f\"FAILED to get LOGS for CONTAINER: {c.name} in POD: {p.metadata.name}\")\n",
                "                    else:\n",
                "                        for line in output.split('\\n'):\n",
                "                            for expression in expressions_to_analyze:\n",
                "                                if expression.match(line):\n",
                "                                    entries_for_analysis.append(line)\n",
                "                            print(line)\n",
                "print(\"\")\n",
                "print(f\"{len(entries_for_analysis)} log entries found for further analysis.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyze log entries and suggest relevant Troubleshooting Guides"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Analyze log entries and suggest further relevant troubleshooting guides\n",
                "\n",
                "from IPython.display import Markdown\n",
                "\n",
                "import os\n",
                "import json\n",
                "import requests\n",
                "import ipykernel\n",
                "import datetime\n",
                "\n",
                "from urllib.parse import urljoin\n",
                "from notebook import notebookapp\n",
                "\n",
                "def get_notebook_name():\n",
                "    \"\"\"Return the full path of the jupyter notebook.   Some runtimes (e.g. ADS) \n",
                "    have the kernel_id in the filename of the connection file.  If so, the \n",
                "    notebook name at runtime can be determined using `list_running_servers`.\n",
                "    Other runtimes (e.g. azdata) do not have the kernel_id in the filename of\n",
                "    the connection file, therefore we are unable to establish the filename\n",
                "    \"\"\"\n",
                "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
                "    \n",
                "    # If the runtime has the kernel_id in the connection filename, use it to\n",
                "    # get the real notebook name at runtime, otherwise, use the notebook \n",
                "    # filename from build time.\n",
                "    try: \n",
                "        kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
                "    except:\n",
                "        pass\n",
                "    else:\n",
                "        for servers in list(notebookapp.list_running_servers()):\n",
                "            try:\n",
                "                response = requests.get(urljoin(servers['url'], 'api/sessions'), params={'token': servers.get('token', '')}, timeout=.01)\n",
                "            except:\n",
                "                pass\n",
                "            else:\n",
                "                for nn in json.loads(response.text):\n",
                "                    if nn['kernel']['id'] == kernel_id:\n",
                "                        return nn['path']\n",
                "\n",
                "def load_json(filename):\n",
                "    with open(filename, encoding=\"utf8\") as json_file:\n",
                "        return json.load(json_file)\n",
                "\n",
                "def get_notebook_rules():\n",
                "    \"\"\"Load the notebook rules from the metadata of this notebook (in the .ipynb file)\"\"\"\n",
                "    file_name = get_notebook_name()\n",
                "\n",
                "    if file_name == None:\n",
                "        return None\n",
                "    else:\n",
                "        j = load_json(file_name)\n",
                "\n",
                "        if \"azdata\" not in j[\"metadata\"] or \\\n",
                "            \"expert\" not in j[\"metadata\"][\"azdata\"] or \\\n",
                "            \"log_analyzer_rules\" not in j[\"metadata\"][\"azdata\"][\"expert\"]:\n",
                "            return []\n",
                "        else:\n",
                "            return j[\"metadata\"][\"azdata\"][\"expert\"][\"log_analyzer_rules\"]\n",
                "\n",
                "rules = get_notebook_rules()\n",
                "\n",
                "if rules == None:\n",
                "    print(\"\")\n",
                "    print(f\"Log Analysis only available when run in Azure Data Studio.  Not available when run in azdata.\")\n",
                "else:\n",
                "    hints = 0\n",
                "    if len(rules) > 0:\n",
                "        for entry in entries_for_analysis:\n",
                "            for rule in rules:\n",
                "                if entry.find(rule[0]) != -1:\n",
                "                    print (entry)\n",
                "\n",
                "                    display(Markdown(f'HINT: Use [{rule[2]}]({rule[3]}) to resolve this issue.'))\n",
                "                    hints = hints + 1\n",
                "\n",
                "    print(\"\")\n",
                "    print(f\"{len(entries_for_analysis)} log entries analyzed (using {len(rules)} rules). {hints} further troubleshooting hints made inline.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clean up temporary directory for staging configuration files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Delete the temporary directory used to hold configuration files\n",
                "\n",
                "import shutil\n",
                "\n",
                "shutil.rmtree(temp_dir)\n",
                "\n",
                "print(f'Temporary directory deleted: {temp_dir}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Notebook execution complete.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Related\n",
                "-------\n",
                "\n",
                "-   [OPR002 - Run\n",
                "    app-deploy](../notebook-o16n/opr002-run-app-deploy.ipynb)\n",
                "\n",
                "-   [OPR600 - Monitor infrastructure\n",
                "    (Kubernetes)](../notebook-o16n/opr600-monitor-infrastructure-k8s.ipynb)\n",
                "\n",
                "-   [OPR900 - Troubleshoot run\n",
                "    app-deploy](../notebook-o16n/opr900-troubleshoot-run-app-deploy.ipynb)"
            ]
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "azdata": {
            "side_effects": true,
            "symlink": true
        }
    }
}